import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint  # æ ¸å¿ƒä¿®æ”¹: å¼•å…¥ checkpoint
from sam2.modeling.fusion import DynamicFusionModule
from sam2.modeling.segformer_head import SegFormerHead


def _get_hiera_dim(trunk):
    """
    æš´åŠ›è·å– Hiera ä¸»å¹²çš„ embed_dimï¼Œå…¼å®¹å„ç§ä¿®æ”¹ç‰ˆ
    """
    # ç­–ç•¥ 1: æŸ¥ PatchEmbed (æœ€ç¨³ï¼Œç›´æ¥çœ‹ç¬¬ä¸€å±‚å·ç§¯è¾“å‡ºå¤šå°‘)
    if hasattr(trunk, 'patch_embed') and hasattr(trunk.patch_embed, 'proj'):
        proj = trunk.patch_embed.proj
        if hasattr(proj, 'out_channels'):  # Conv2d
            return proj.out_channels
        if hasattr(proj, 'out_features'):  # Linear
            return proj.out_features

    # ç­–ç•¥ 2: æŸ¥ PosEmbed (æ¬¡ç¨³ï¼Œçœ‹ä½ç½®ç¼–ç çš„æœ€åä¸€ç»´)
    if hasattr(trunk, 'pos_embed') and trunk.pos_embed is not None:
        return trunk.pos_embed.shape[-1]

    # ç­–ç•¥ 3: æŸ¥ Blocks çš„ Norm å±‚
    if hasattr(trunk, 'blocks') and len(trunk.blocks) > 0:
        b0 = trunk.blocks[0]
        # åªè¦æ˜¯ LayerNormï¼Œå°±æœ‰ normalized_shape
        if hasattr(b0, 'norm1') and hasattr(b0.norm1, 'normalized_shape'):
            return b0.norm1.normalized_shape[0]

    # ç­–ç•¥ 4: æŸ¥å¸¸è§å±æ€§å
    for attr in ['embed_dim', 'dim', 'd_model', 'num_features']:
        if hasattr(trunk, attr):
            return getattr(trunk, attr)

    raise AttributeError("æ— æ³•è‡ªåŠ¨æ£€æµ‹ Hiera çš„ç»´åº¦ï¼è¯·æ£€æŸ¥ backbone ç»“æ„ã€‚")


class SerialSAM2Backbone(nn.Module):
    """
    ã€å…±äº« MoE ç‰ˆã€‘ä¸²è¡Œéª¨å¹²ç½‘
    ç»“æ„ï¼šHiera Stage -> Shared MoE -> Hiera Stage -> ...
    RGB å’Œ IR å…±ç”¨åŒä¸€ç»„ MoE å‚æ•°ã€‚
    """

    def __init__(self, base_sam, moe_class, feature_channels=None, num_experts=8, active_experts=3):
        super().__init__()
        self.base_sam = base_sam

        # 1. è‡ªåŠ¨æ£€æµ‹ç‰¹å¾ç»´åº¦ (ä½¿ç”¨æš´åŠ›æ£€æµ‹å‡½æ•°)
        if feature_channels is None:
            embed_dim = _get_hiera_dim(base_sam.image_encoder.trunk)

            # Hiera çš„å±‚çº§å€ç‡é€šå¸¸æ˜¯ [1, 2, 4, 8]
            feature_channels = [embed_dim * (2 ** i) for i in range(4)]
            print(f"ğŸ”§ [Auto-Detect] Backbone Embed Dim: {embed_dim} (Checkpointing Enabled)")
            print(f"ğŸ”§ [Auto-Detect] Feature Channels: {feature_channels}")

        # 2. å†»ç»“åŸå§‹ SAM2
        for param in self.base_sam.parameters():
            param.requires_grad = False

        # 3. æ„å»ºå…±äº«çš„ä¸²è¡Œ MoE å±‚ (Shared MoE)
        self.shared_moe_layers = nn.ModuleList([
            moe_class(dim=ch, num_experts=num_experts, active_experts=active_experts)
            for ch in feature_channels
        ])

    def run_serial_stream(self, image, moe_layers):
        """ å•æ¨¡æ€ä¸²è¡Œå‰å‘ä¼ æ’­ """
        trunk = self.base_sam.image_encoder.trunk

        # --- 1. Patch Embed & Pos Embed ---
        x = trunk.patch_embed(image)

        if trunk.pos_embed is not None:
            pos_embed = trunk.pos_embed
            B, H, W, C = x.shape
            # è‡ªåŠ¨å°ºå¯¸é€‚é…
            if pos_embed.shape[-1] != C and pos_embed.shape[1] == C:
                pos_embed = pos_embed.permute(0, 2, 3, 1)
            if pos_embed.shape[1] != H or pos_embed.shape[2] != W:
                pos_embed = pos_embed.permute(0, 3, 1, 2)
                pos_embed = F.interpolate(pos_embed, size=(H, W), mode='bilinear', align_corners=False)
                pos_embed = pos_embed.permute(0, 2, 3, 1)
            x = x + pos_embed

        features = []
        total_aux_loss = 0.0

        # --- 2. é€ Block è¿è¡Œ ---
        stage_idx = 0
        for i, blk in enumerate(trunk.blocks):
            # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing) â˜…â˜…â˜…
            # åªæœ‰å½“ x éœ€è¦æ¢¯åº¦æ—¶ï¼ˆå³å·²ç»è¿‡äº†ç¬¬ä¸€ä¸ª MoE å±‚ï¼‰ï¼Œå¼€å¯ checkpoint æ‰æœ‰æ„ä¹‰
            # è¿™å¯¹äºå†»ç»“çš„ backbone å°¤å…¶é‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼
            if self.training and x.requires_grad:
                # use_reentrant=False æ˜¯æ–°ç‰ˆ PyTorch æ¨èçš„ï¼Œæ›´å®‰å…¨
                x = checkpoint(blk, x, use_reentrant=False)
            else:
                x = blk(x)

            # B. æ£€æŸ¥ Stage ç»“å°¾ï¼Œæ’å…¥ MoE
            if i in trunk.stage_ends:
                # 1. è·‘ MoE (å…±äº«å‚æ•°)
                x_in = x.permute(0, 3, 1, 2)
                x_out, aux_loss = moe_layers[stage_idx](x_in)
                total_aux_loss += aux_loss

                # 2. æ®‹å·®è¿æ¥
                x = x + x_out.permute(0, 2, 3, 1)

                features.append(x.permute(0, 3, 1, 2))
                stage_idx += 1

        return features, total_aux_loss

    def forward(self, img_rgb, img_ir):
        feats_rgb, loss_rgb = self.run_serial_stream(img_rgb, self.shared_moe_layers)
        feats_ir, loss_ir = self.run_serial_stream(img_ir, self.shared_moe_layers)
        return feats_rgb, feats_ir, (loss_rgb + loss_ir)


class SerialSegModel(nn.Module):
    """
    åŸºç¡€åˆ†å‰²æ¨¡å‹
    """

    def __init__(self, base_sam, moe_class, num_classes=9):
        super().__init__()
        # 1. åˆå§‹åŒ– Backbone (è‡ªåŠ¨æ£€æµ‹ç»´åº¦)
        self.backbone = SerialSAM2Backbone(base_sam, moe_class, feature_channels=None)

        # 2. è·å–æ£€æµ‹åˆ°çš„ç»´åº¦ (å¤ç”¨æ£€æµ‹é€»è¾‘)
        embed_dim = _get_hiera_dim(base_sam.image_encoder.trunk)
        channels = [embed_dim * (2 ** i) for i in range(4)]

        # 3. åˆå§‹åŒ–åç»­å±‚
        self.fusion_layers = nn.ModuleList([DynamicFusionModule(ch) for ch in channels])

        # â˜…â˜…â˜… ç¡®ä¿ SegFormerHead è¢«åˆå§‹åŒ– â˜…â˜…â˜…
        self.segformer_head = SegFormerHead(in_channels=channels, num_classes=num_classes)

    def forward(self, vis, ir, gt_entropy_maps=None):
        feats_rgb, feats_ir, moe_loss = self.backbone(vis, ir)

        fused_features = []
        total_fusion_loss = 0.0

        for i in range(4):
            gt_ent = gt_entropy_maps[i] if (gt_entropy_maps is not None) else None
            f_out, f_loss = self.fusion_layers[i](feats_ir[i], feats_rgb[i], gt_ent)
            fused_features.append(f_out)
            total_fusion_loss += f_loss

        logits = self.segformer_head(fused_features)
        logits = F.interpolate(logits, size=vis.shape[2:], mode='bilinear')

        return logits, moe_loss, total_fusion_loss